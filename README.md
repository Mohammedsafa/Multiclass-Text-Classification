# ğŸ§  Multiclass Text Classification with Transformers and Pretrained Embeddings

A deep learning project for classifying Arabic/English news articles into five categories: **Politics**, **Sports**, **Media**, **Market & Economy**, and **STEM**. This project investigates the impact of different embedding techniques, model architectures, and regularization strategies on classification performance.

---

## ğŸ“Œ Table of Contents

- [ğŸ“Œ Table of Contents](#-table-of-contents)
- [ğŸ“‚ Project Overview](#-project-overview)
- [ğŸ› ï¸ Model Architectures](#ï¸-model-architectures)
- [ğŸ§ª Experiments & Results](#-experiments--results)
- [ğŸ¯ Regularization Techniques](#-regularization-techniques)
- [ğŸš€ Inference Example](#-inference-example)
- [ğŸ“Š Evaluation](#-evaluation)
- [ğŸ‘¨â€ğŸ’» Author](#-author)
- [ğŸ§¾ Project Context](#-project-context)
- [ğŸ“š References and Acknowledgments](#-references-and-acknowledgments)

---

## ğŸ“‚ Project Overview

We explored various embeddings and architectures for text classification. Our primary goal was to improve validation accuracy while keeping the model efficient and generalizable.

---

## ğŸ› ï¸ Model Architectures

### 1. âœ… Final Model (Best Performance)

RoBERTa Embedding â†’ Dropout â†’ Transformer Encoder â†’ BiGRU â†’ Fully Connected â†’ Softmax


- Pretrained RoBERTa embeddings (frozen)
- Transformer encoder layer with GELU and LayerNorm
- Bidirectional GRU layer for temporal learning
- Fully connected output layer for classification

---

## ğŸ§ª Experiments & Results

| Embedding Type         | Model Architecture          | Validation Accuracy |
|------------------------|-----------------------------|---------------------|
| Trainable Embedding    | LSTM + Fully Connected      | 70%                 |
| GloVe Pretrained       | Transformer + FC Layers     | 74.25%              |
| **RoBERTa Pretrained** | Transformer Encoder + BiGRU | **77.15%**          |

---

## ğŸ¯ Regularization Techniques

To prevent overfitting and improve generalization:

- **Label Smoothing**: Makes model less confident, improves generalization.
- **Class Weights**: Balances loss impact of underrepresented classes.
- **Dropout**: Applied after embedding layer.
- **Early Stopping**: Stops training when validation acc stops improving.

> ğŸ“Œ **Note**: Training loss is **higher than validation loss** due to regularization methods.

---

## ğŸš€ Inference Example

```python
text = "Managing cash flow effectively is crucial for any business, ensuring that expenses donÃƒÂ¢Ã¢,Â¬Ã¢â€Â¢t exceed income."
input_ids, mask = prepare_single_input(text, tokenizer, max_len=120)
output = model(input_ids, mask)
pred = torch.argmax(output, dim=1)
print("Predicted Class:", label_map[int(pred.item())])
The prediction class is:  Market & Economy
```

## ğŸ“Š Evaluation

 - Metrics: Accuracy, Macro F1-score, Confusion Matrix

 - Visualized using Seaborn heatmaps

 - Inference tested on unseen examples

## ğŸ‘¨â€ğŸ’» Author
Developed by Mohammed Safa for academic and purposes.


## ğŸ§¾ Project Context
This project was initially developed as part of a university semester course. After the course ended, additional enhancements were made to improve performance and generalization. It serves as a practical exploration of embedding techniques, model design, and training strategies for natural language classification.

## ğŸ“š References & Acknowledgments
We acknowledge the use of the following tools and resources:

 - RoBERTa pretrained models from HuggingFace ğŸ¤— Transformers.

 - GloVe embeddings from the Stanford NLP Group.

 - HuggingFace and PyTorch documentation.

 - Research inspiration from:

    - Vaswani et al., â€œAttention is All You Needâ€ (2017).

    - Relevant academic blogs and GitHub repositories.

This repository is for educational and research purposes only.
